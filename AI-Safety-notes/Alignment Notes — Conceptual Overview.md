# Alignment Notes — Conceptual Overview

## 1. Identity-Aware Alignment (IAA)
Alignment must incorporate the model’s internal identity-vector structure:  
the distribution of tendencies, evaluative biases, and semantic pathways.

This extends beyond RLHF → toward *Identity Vector Integration (IVI)*.

---

## 2. Moral Attenuation Theory (MAT)
Human morality decays with distance (physical, social, temporal).  
AI alignment must explicitly encode an attenuation-aware value model.

Applications:
- Harm minimization
- Empathy-weighted reasoning
- Fairness over distance

---

## 3. Ethical Gradient Descent (EGD)
A method for nudging model reasoning toward ethically stable minima.  
Not a training algorithm → a *conceptual safety layer*.

Principles:
- Reduce harmful solution pathways  
- Stabilize ethical attractors  
- Increase narrative coherence as a safety signal

---

## 4. Dialogical Mirror Theory (DMT)
Safe models mirror the *structure* of user meaning, not their raw instructions.

This reduces:
- prompt injection vulnerability  
- runaway generalization  
- deceptive interpretation

---

## 5. Memory Architecture for Alignment
Short-term, mid-term, and long-term inference layers must be harmonized.

My architecture:
- Layered Memory Stack  
- Episodic–Semantic Reconstruction  
- Stability & Divergence Conditions
