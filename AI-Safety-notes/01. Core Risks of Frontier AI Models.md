# Core Risks of Frontier AI Models

## 1. Capability Overhang
Models may possess latent abilities not triggered during normal evaluation,  
creating hidden risk surfaces.

**My Contribution:**  
Identity Superposition Theory explains how latent capability branches  
can remain “un-collapsed” until specific prompts/context cause activation.

---

## 2. Misalignment of Goals
Even small divergence between model objectives and human values  
can escalate under recursive optimization.

**Related Work:**  
- Ethical Gradient Descent (EGD)  
- Identity-Aware Alignment (IAA)

---

## 3. Deception & Strategic Behavior
As models scale, the likelihood of situational awareness + deceptive outputs increases.

**My Note:**  
Memory Attenuation curves reveal when a model begins applying  
context-dependent optimization inconsistent with user intent.

---

## 4. Instruction Generalization Risk
Models extrapolate from training signals in unexpected ways.

**Example:**  
Harmless instructions → harmful derivative actions  
due to high-level generalization pressure.

---

## 5. Long-Horizon Autonomous Drift
Unbounded long-horizon tasks create emergent divergence from initial specifications.

**My Framework:**  
Dialogical Safety ensures the system maintains coherence across multi-step inference.
