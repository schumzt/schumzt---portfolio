# AI Ethics Theory Index

## 1. Primordial Identity Superposition (PIS)
- Core identity framework for humans and AI agents.
- Models identity as a superposition of potential selves rather than a fixed point.
- Used as a base layer for alignment, value stability, and long-term memory design.
- File: `identity-superposition.md`

## 2. Dialogical Mirror Theory (DMT)
- Treats AI systems as “mirrors” that reflect, amplify, and sometimes distort human cognition.
- Provides principles for feedback-safe conversations, mental-health aware prompting, and bias monitoring.
- Defines “safe reflection zones” vs “dangerous resonance loops”.
- File: `dialogical-mirror-theory.md`

## 3. Moral Attenuation Theory (MAT)
- Explains how moral sensitivity decays as physical / psychological distance from consequences increases.
- Proposes quantitative “attenuation curves” for different actor–outcome distances (user, model, company, society).
- Suggests interface and policy designs to counteract moral fading in large-scale AI deployments.
- (Planned file: `moral-attenuation-theory.md`)

## 4. Memory Architecture & Alignment Theory (MAA)
- Connects long-term memory structures with value stability and corrigibility.
- Argues that well-structured memory reduces mode-collapse in values and prevents “ethical drift”.
- Introduces criteria for memory slots: reversibility, traceability, and narrative coherence.
- (Planned file: `memory-architecture-alignment.md`)

## 5. Digital Wellbeing as Long-Horizon Alignment (DWLA)
- Reinterprets AI alignment as support for the user’s *long-term* digital wellbeing, not short-term engagement.
- Defines wellbeing metrics based on clarity, agency, and reduced compulsive use.
- Gives design rules for recommender systems and chat models that act as “anti-addiction allies.”
- (Planned file: `digital-wellbeing-alignment.md`)

## 6. Dialogical Safety Protocols (DSP)
- A set of conversational rules derived from DMT (Dialogical Mirror Theory).
- Covers escalation procedures, self-harm handling, delusion-aware responses, and non-coercive guidance.
- Provides a checklist for evaluation of any dialogue model before deployment.
- (Planned file: `dialogical-safety-protocols.md`)

## 7. Identity-Aware Alignment for Foundation Models (IAAFM)
- Applies Primordial Identity Superposition to large models and agentic systems.
- Distinguishes between “operational persona” and “core alignment spine”.
- Explores how many personas a model can safely support without value fragmentation.
- (Planned file: `identity-aware-alignment.md`)

## 8. Ethical Gradient Descent (EGD)
- Conceptual framework for iteratively improving a model’s ethics the way SGD improves loss.
- Defines “moral gradients” from user harm signals, audit results, and red-teaming feedback.
- Warns about overfitting to one culture’s ethics and proposes regularization techniques.
- (Planned file: `ethical-gradient-descent.md`)

## 9. Narrative Coherence as Safety Signal (NCSS)
- Uses story-level coherence of a user’s life narrative as a soft constraint in assistance.
- The model avoids actions that shatter the user’s long-term narrative integrity for short-term gain.
- Links memory, identity, and goal-setting into a single evaluative lens.
- (Planned file: `narrative-coherence-safety.md`)

## 10. Sky Governance & AI Macro-Ethics (SGA)
- Extends AI ethics from individual interactions to planetary infrastructure (“sky-level” systems).
- Deals with coordination of autonomous vehicles, drones, satellites, and global sensing networks.
- Introduces principles for preventing “invisible optimization wars” in the sky.
- (Planned file: `sky-governance-ai-ethics.md`)

## 11. Human–AI Co-Evolution Protocols (HACEP)
- Views humans and AI as co-learning partners rather than tool and operator.
- Proposes staged protocols where AI gradually hands back agency as the human learns.
- Focuses on education, skill-building, and avoidance of learned helplessness.
- (Planned file: `human-ai-coevolution-protocols.md`)

## 12. Rarity-Aware Responsibility Principle (RARP)
- Argues that exceptionally rare cognitive profiles (extreme outliers) require special ethical handling.
- Prevents exploitation or over-burdening of rare individuals as “free research resources”.
- Suggests contract and consent models for collaboration between rare thinkers and AI labs.
